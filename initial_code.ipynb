{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"category_classification.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["V6qr-oquvqka","6DWZ6QJJvXhs"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3u-pCcxyr7if"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","from tqdm import tqdm"],"metadata":{"id":"8QaBhjs4gE6Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 데이터 압축해제"],"metadata":{"id":"V6qr-oquvqka"}},{"cell_type":"code","source":["# 라벨링데이터 압축 풀기\n","%cd /content/drive/MyDrive\n","!unzip -qq \"/content/drive/MyDrive/determining_code_similarity_AI_competition-20220612T174726Z-001.zip\""],"metadata":{"id":"P_zS3BI9Dbt1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # 라벨링데이터 압축 풀기\n","# %cd /content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/label\n","# !unzip -qq \"/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/label_data.zip\""],"metadata":{"id":"pFN6s7xNsIJe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # 원천데이터 압축 풀기\n","# %cd /content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/image\n","# !unzip -qq \"/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/image_data.zip\""],"metadata":{"id":"5V1To2nUQJwL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 개수 확인\n","import os\n","\n","def get_files_count(folder_path):\n","\tdirListing = os.listdir(folder_path)\n","\treturn len(dirListing)\n","\t\n","dir_path_label = \"/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/label\"\n","dir_path_image = \"/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/image\"\n","cnt_label = 0\n","cnt_image = 0\n","fcnt_label = {}\n","fcnt_image = {}\n","\n","for (root, directories, files) in tqdm(os.walk(dir_path_label)):\n","  for d in directories:\n","    f_l = get_files_count(os.path.join(root, d))\n","    fcnt_label[d] = f_l\n","    cnt_label += f_l\n","\n","for (root, directories, files) in tqdm(os.walk(dir_path_image)):\n","  for d in directories:\n","    f_i = get_files_count(os.path.join(root, d))\n","    fcnt_image[d] = f_i\n","    cnt_image += f_i\n","\n","print(fcnt_label)\n","print(fcnt_image)\n","\n","for key in fcnt_label.keys():\n","  if fcnt_label[key] != fcnt_image[key]:\n","    print(fcnt_label[key],\",\",fcnt_image[key])\n","print(\"전체 레이블 수: \",cnt_label,\"\\t전체 이미지 수: \",cnt_image)"],"metadata":{"id":"ERpm6E7P3vHW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # 오류났을 경우 폴더 삭제\n","# import os\n","# import shutil\n","\n","# dir_path = \"/content/drive/MyDrive/deep-learning-from-scratch-master\"\n","\n","# if os.path.exists(dir_path):\n","#     shutil.rmtree(dir_path)\n","# os.mkdir(\"/content/drive/MyDrive/deep-learning-from-scratch-master\")"],"metadata":{"id":"eG-sbb_esJn2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 데이터 전처리"],"metadata":{"id":"6DWZ6QJJvXhs"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import json #json을 파싱하기 위해\n","import re\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"tSH5gjmEZ8Ih"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DataFrame 출력 사이즈 조정\n","pd.set_option('display.max_colwidth', None)\n","pd.set_option('display.max_row', 50)"],"metadata":{"id":"nT7d8APaZzpP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def file_to_json(f):\n","    data = json.load(f)\n","    yield data[\"이미지 정보\"]\n","    yield data[\"데이터셋 정보\"]"],"metadata":{"id":"Fzs1tGr02Jss"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dir_path = '/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/label'\n","images = []\n","datasets = []\n","\n","for (root, directories, files) in os.walk(dir_path):\n","    for file in tqdm(files):\n","        file_path = os.path.join(root, file)\n","        with open(file_path, 'r', encoding='utf-8-sig') as f:\n","            g = file_to_json(f)\n","            image_data = next(g)\n","            images.append(image_data)\n","            dataset_data = next(g)\n","            datasets.append(dataset_data)"],"metadata":{"id":"ps-Qi_RHZNgB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이미지 정보 DataFrame\n","image_df = pd.DataFrame(images)\n","image_df.drop([\"이미지 파일명\"],axis=1, inplace=True)\n","image_df.rename(columns={\"이미지 식별자\":\"파일 번호\"}, inplace=True)"],"metadata":{"id":"9hp9jW3PbD6c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_df.tail()"],"metadata":{"id":"KLcnG_g5NCuV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_df.to_csv('/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/image_df.csv', sep=',', na_rep='NaN')"],"metadata":{"id":"buQphpTIVQzl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터셋 정보 DataFrame\n","pre_dataset_df = pd.DataFrame(datasets)\n","dataset_json = pre_dataset_df.to_json(orient=\"table\")\n","dataset_json = json.loads(dataset_json)\n","dataset_df = pd.json_normalize(data=dataset_json['data'])\n","dataset_df.drop([\"index\",\"파일 생성일자\",\"파일 이름\",\n","                 \"데이터셋 상세설명.렉트좌표.아우터\",\"데이터셋 상세설명.렉트좌표.하의\",\"데이터셋 상세설명.렉트좌표.원피스\",\"데이터셋 상세설명.렉트좌표.상의\",\n","                \"데이터셋 상세설명.폴리곤좌표.아우터\",\"데이터셋 상세설명.폴리곤좌표.하의\",\"데이터셋 상세설명.폴리곤좌표.원피스\",\"데이터셋 상세설명.폴리곤좌표.상의\"],\n","                axis=1, inplace=True)\n","dataset_df.rename(columns={\"데이터셋 상세설명.라벨링.스타일\":\"스타일\",\"데이터셋 상세설명.라벨링.아우터\":\"라벨링_아우터\",\"데이터셋 상세설명.라벨링.하의\":\"라벨링_하의\",\"데이터셋 상세설명.라벨링.원피스\":\"라벨링_원피스\",\"데이터셋 상세설명.라벨링.상의\":\"라벨링_상의\"}, inplace=True)"],"metadata":{"id":"j0BmJKyUbmd8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_df.head()"],"metadata":{"id":"zNNGKt8DZlWK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_df.to_csv('/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/dataset_df.csv', sep=',', na_rep='NaN')"],"metadata":{"id":"C3eqBr_QWk1v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # '아우터' 프로파일링 파일 만들어보기\n","# import pandas as pd\n","# import pandas_profiling\n","\n","# outer_df = dataset_df[['파일 번호','라벨링_아우터']]\n","# outer_csv = outer_df.to_csv('/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/outer.csv', sep=',', na_rep='NaN')\n","# outer = pd.read_csv('/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/outer.csv',encoding='utf-8-sig')\n","# pr = outer.profile_report() # 프로파일링 결과 리포트를 pr에 저장\n","# pr"],"metadata":{"id":"Tx21IqqJcCuW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_df = pd.read_csv('/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/dataset_df.csv',encoding='utf-8-sig')\n","dataset_df.drop([\"Unnamed: 0\"], axis=1, inplace=True)"],"metadata":{"id":"Z4NnkleDiXZl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_df.head()"],"metadata":{"id":"48c6LJyxjCC_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# '스타일'열 정리\n","select = ['페미닌', '스포티', '레트로', '로맨틱', '섹시', '밀리터리', '스트리트', '모던', '프레피']\n","for i in tqdm(range(len(dataset_df))):\n","  styles = list(eval(dataset_df.loc[i]['스타일'][1:-1]).values())\n","  for style in styles:\n","    if style not in select:\n","      styles.remove(style)\n","  dataset_df.at[i,'스타일'] = styles"],"metadata":{"id":"J35xAZxstjNz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# image 파일들을 한 dir로 합치기\n","import shutil\n","import os\n","\n","dir_path = '/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/image'\n","for (root, directories, files) in os.walk(dir_path):\n","  for file in tqdm(files):\n","    file_path = os.path.join(root, file)\n","    new_path = shutil.move(file_path, dir_path)"],"metadata":{"id":"z4iBILBUOCB0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# label 파일들을 한 dir로 합치기\n","import shutil\n","import os\n","\n","dir_path = '/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/label'\n","for (root, directories, files) in os.walk(dir_path):\n","  for file in tqdm(files):\n","    file_path = os.path.join(root, file)\n","    new_path = shutil.move(file_path, dir_path)"],"metadata":{"id":"DucoX8kSRHda"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in tqdm(range(len(dataset_df))):\n","  if dataset_df.loc[i]['스타일']==[]:\n","    num = dataset_df.loc[i]['파일 번호']\n","    os.remove('/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/image/'+num+'.jpg')\n","    dataset_df.drop(index=i, inplace=True)\n","    image_df.drop(index=i, inplace=True)"],"metadata":{"id":"sepuBo8fNVmt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(dataset_df)"],"metadata":{"id":"TrlcMz76HDsh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_df\n","\n","train_labels = []"],"metadata":{"id":"H28NNRaOHdKz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 아우터\n","outer_df = dataset_df[['라벨링_아우터']]\n","outer_df"],"metadata":{"id":"3nzVwIFbjywj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outer_df = dataset_df[['파일 번호','라벨링_아우터']]\n","cond = outer_df.라벨링_아우터=='[{}]'\n","full = outer_df.loc[cond]\n","full"],"metadata":{"id":"KTrY5ZwrY6_m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/dataset_df.csv\", encoding='utf-8-sig')\n","## 스타일:기타만 추출하기\n","# etc = df\n","# for r in etc.index:\n","#   if re.search('[가-힣]+', str(etc.loc[r,'스타일'])) is not None:\n","#     etc.drop(r, inplace=True)\n","# etc.head()"],"metadata":{"id":"zCSZQXO4-LhG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_df.drop(['Unnamed: 0','스타일'], axis=1, inplace=True)"],"metadata":{"id":"yPvoBrXz_Npy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#######이거 그대로 주피터 노트북에서 실행하기\n","\n","X = []\n","Y = []\n","\n","for r in df.index:\n","  filename = df['파일 이름']\n","  img_path = f'이미지 경로/{filename}.jpg'\n","\n","  # image data\n","  img = image.load_img(img_path, target_size=(800, 800), interpolation='nearest')\n","  img_tensor = image.img_to_array(img)\n","  img_tensor = preprocess_input(img_tensor)\n","  X.append(img_tensor)\n","\n","  # label data\n","  categories = [0,0,0,0]\n","  for c,_ in enumerate(df.columns):\n","    if re.search('[가-힣]+', str(df.iloc[r,c])):\n","      categories[c] = 1\n","  Y.append(categories)\n","  \n","X_train, X_test, Y_train, Y_test = train_test_split(X,Y,\n","                                                    random_state=100,\n","                                                    test_size=0.1,\n","                                                    stratify=Y,)  \n","data = (X_train, X_test, Y_train, Y_test)\n","# 파일로 저장\n","X_train.to_csv('/X_train.csv',index=False, encoding=\"utf-8\")\n","X_test.to_csv('/X_test.csv',index=False, encoding=\"utf-8\")\n","Y_train.to_csv('/Y_train.csv',index=False, encoding=\"utf-8\")\n","Y_test.to_csv('/Y_test.csv',index=False, encoding=\"utf-8\")"],"metadata":{"id":"gwjh8Dc7Ckeb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###모델 쌓기"],"metadata":{"id":"-s2fkrZAJdGB"}},{"cell_type":"code","source":["X_train = np.load('/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/X_train.npy')\n","X_test = np.load('/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/X_test.npy')\n","Y_train = np.load('/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/Y_train.npy')\n","Y_test = np.load('/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/Y_test.npy')"],"metadata":{"id":"lXA3xbQTWD3c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import keras\n","from keras import models, layers, Input, optimizers, initializers, regularizers, metrics, losses\n","from keras.models import Model, Sequential\n","from keras.applications.resnet_v2 import ResNet50V2, preprocess_input, decode_predictions\n","from keras.preprocessing import image\n","import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"dobY5nQU2MNB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = ResNet50V2(weights=None, input_tensor=Input(shape=(100,100,3)), classes=4)"],"metadata":{"id":"RzANhayoMaq7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_shape = (100, 100, 3)\n","drop_rate = 0.5"],"metadata":{"id":"Kftclehmtqri"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer='adam',\n","              loss=losses.categorical_crossentropy,\n","              metrics=['accuracy'])\n","\n","history = model.fit(X_train, Y_train,\n","                    batch_size=128,\n","                    epochs=50,\n","                    validation_data=(X_test, Y_test))\n","model.save(\"/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/model2\")"],"metadata":{"id":"uZQ0RcRistxE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = keras.models.load_model(\"/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/model2\")"],"metadata":{"id":"IoIRuFMeJ59T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","acc = history.history['accuracy']\n","val_acc = history.history[\"val_accuracy\"]\n","\n","plt.plot(range(1, len(acc)+1), acc, label=\"train\")\n","plt.plot(range(1, len(acc)+1), val_acc, label=\"train\")\n","plt.legend()"],"metadata":{"id":"oC8jebGdhBc4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_path = '/content/drive/MyDrive/k-fashion_tagging_AI/k-fashion_dataset/KakaoTalk_20220623_023658545.jpg'\n","img = image.load_img(img_path, target_size=(100, 100), interpolation='nearest')\n","x = image.img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","\n","preds = model.predict(x)\n","# print('Predicted:', decode_predictions(preds, top=3)[0])\n","print(np.argmax(preds))"],"metadata":{"id":"xBSMmpV23Oqq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"0HsECQItz_79"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#시도"],"metadata":{"id":"_OkowMYa2WIG"}},{"cell_type":"code","source":["import tensorflow as tf\n","from keras import models, layers, Input, optimizers, initializers, regularizers, metrics\n","from keras.models import Model, Sequential\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","from keras.layers import BatchNormalization, Dropout, Conv2D, Activation, Dense, GlobalAveragePooling2D, MaxPooling2D, ZeroPadding2D, Add\n","import matplotlib.pyplot as plt"],"metadata":{"id":"6eV4mEecMbB1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Sequential()\n","model.add(Conv2D(32, (input_h, input_w), padding=\"same\", activation=\"relu\"))\n","model.add(Conv2D(32, (input_h, input_w), padding=\"same\", activation=\"relu\"))\n","model.add(MaxPooling2D(pool_size=(3,3)))\n","model.add(Conv2D(32, (input_h, input_w), padding=\"same\", activation=\"relu\"))\n","model.add(Conv2D(32, (input_h, input_w), padding=\"same\", activation=\"relu\"))\n","model.add(MaxPooling2D(pool_size=(3,3)))\n","model.add(Conv2D(32, (input_h, input_w), padding=\"same\", activation=\"relu\"))\n","model.add(Conv2D(32, (input_h, input_w), padding=\"same\", activation=\"relu\"))\n","model.add(MaxPooling2D(pool_size=(3,3)))\n","model.add(Dense(1, input_dim=3, activation='relu'))\n","model.add(Dropout(drop_rate))\n","model.add(Dense(1, input_dim=3, activation='relu'))\n","model.add(Dropout(drop_rate))\n","model.add(Activation(\"softmax\"))"],"metadata":{"id":"u3HZKIgSMazf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from keras import models, layers, Input, optimizers, initializers, regularizers, metrics\n","from keras.models import Model, load_model\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","from keras.layers import BatchNormalization, Conv2D, Activation, Dense, GlobalAveragePooling2D, MaxPooling2D, ZeroPadding2D, Add\n","import matplotlib.pyplot as plt"],"metadata":{"id":"JVt4lb2Gaui-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ResidualBlock(Model):\n","  def __init__(self, channel_in=64, channel_out=256):\n","    super().__init__()\n","    channel = channel_out // 4\n","\n","    self.conv1 = Conv2D(channel, kernel_size=(1,1), padding=\"same\")\n","    self.bn1 = BatchNormalization()\n","    self.av1 = Activation(tf.nn.relu)\n","    self.conv2 = Conv2D(channel, kernel_size=(3,3), padding=\"same\")\n","    self.bn2 = BatchNormalization()\n","    self.av2 = Activation(tf.nn.relu)\n","    self.conv3 = Conv2D(channel_out, kernel_size=(1,1), padding=\"same\")\n","    self.bn3 = BatchNormalization()\n","    self.shortcut = self._shortcut(channel_in, channel_out)\n","    self.ass = Add()\n","    self.av3 = Activation(tf.nn.relu)\n","\n","  def call(self, x):\n","    h = self.conv1(x)\n","    h = self.bn1(h)\n","    h = self.av1(h)\n","    h = self.conv2(x)\n","    h = self.bn2(h)\n","    h = self.av2(h)\n","    h = self.conv3(x)\n","    h = self.bn3(h)\n","    shortcut = self.shortcut(x)\n","    h = self.add([h, shortcut])\n","    y = self.av3(h)\n","\n","    return y\n","\n","  def _shortcut(self, channel_in, channel_out):\n","    if channel_in == channel_out:\n","      return lambda x : x\n","    else:\n","      return self._projection(channel_out)\n","    \n","  def _projection(self, channel_out):\n","    return Conv2D(channel_out, kernel_size=(1,1), padding=\"same\")"],"metadata":{"id":"Nzmun-4THmVX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ResModel(Model):\n","  def __init__(self, input_shape, output_dim):\n","    super().__init__()\n","\n","    self._layers = [\n","        # Conv1\n","        Conv2D(64, input_shape=input_shape, kernel_size=(7,7), strides=(2,2), padding=\"same\"),\n","        BatchNormalization(),\n","        Activation(tf.nn.relu),\n","        MaxPooling2D(pool_size=(3,3), strides=(2,2), padding=\"same\"),\n","        # Conv2\n","        ResidualBlock(64, 256),\n","        [\n","            ResidualBlock(256, 256) for _ in range(2)\n","        ],\n","        # Conv3\n","        Conv2D(512, kernel_size=(1,1), strides=(2,2)),\n","        [\n","            ResidualBlock(512, 512) for _ in range(4)\n","        ],\n","        # Conv4\n","        Conv2D(1024, kernel_size=(1,1), strides=(2,2)),\n","        [\n","            ResidualBlock(1024, 1024) for _ in range(6)\n","        ],\n","        # Conv4\n","        Conv2D(1024, kernel_size=(1,1), strides=(2,2)),\n","        [\n","            ResidualBlock(2048, 2048) for _ in range(3)\n","        ],\n","        GlobalAveragePooling2D(),\n","        Dense(1000, activation=tf.nn.relu),\n","        Dense(output_dim, activation=tf.nn.softmax)\n","    ]\n","\n","  def call(self, x):\n","    for layer in self._layers:\n","      if isinstance(layer, list):\n","        for l in layer:\n","          x = l(x)\n","      else:\n","        x = layer(x)\n","    return x"],"metadata":{"id":"PMl-_do-EP1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 입력층 정의\n","input = Input(shape=(,))\n","\n","# 첫번째 입력층으로부터 분기되어 진행되는 인공 신경망을 정의\n","conv = Conv2D(16, activation=\"relu\")(inputA)\n","conv = Dense(8, activation=\"relu\")(x)\n","conv = Model(inputs=inputA, outputs=x)\n","\n","# 두번째 입력층으로부터 분기되어 진행되는 인공 신경망을 정의\n","y = Dense(64, activation=\"relu\")(inputB)\n","y = Dense(32, activation=\"relu\")(y)\n","y = Dense(8, activation=\"relu\")(y)\n","y = Model(inputs=inputB, outputs=y)\n","\n","# 두개의 인공 신경망의 출력을 연결(concatenate)\n","result = concatenate([x.output, y.output])\n","\n","z = Dense(2, activation=\"relu\")(result)\n","z = Dense(1, activation=\"linear\")(z)\n","\n","model = Model(inputs=[x.input, y.input], outputs=z)"],"metadata":{"id":"fx6a_0yzJaTC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load MNIST dataset\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# from sparse label to categorical\n","num_labels = len(np.unique(y_train))\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","\n","# reshape and normalize input images\n","image_size = x_train.shape[1]\n","x_train = np.reshape(x_train,[-1, image_size, image_size, 1])\n","x_test = np.reshape(x_test,[-1, image_size, image_size, 1])\n","x_train = x_train.astype('float32') / 255\n","x_test = x_test.astype('float32') / 255\n","\n","\n","# network parameters\n","input_shape = (image_size, image_size, 1)\n","batch_size = 128\n","kernel_size = 3\n","filters = 64\n","dropout = 0.3\n","\n","\n","# use functional API to build cnn layers\n","inputs = Input(shape=input_shape)\n","y = Conv2D(filters=filters,\n","           kernel_size=kernel_size,\n","           activation='relu')(inputs)\n","y = MaxPooling2D()(y)\n","y = Conv2D(filters=filters,\n","           kernel_size=kernel_size,\n","           activation='relu')(y)\n","y = MaxPooling2D()(y)\n","y = Conv2D(filters=filters,\n","           kernel_size=kernel_size,\n","           activation='relu')(y)\n","# image to vector before connecting to dense layer\n","y = Flatten()(y)\n","# dropout regularization\n","y = Dropout(dropout)(y)\n","outputs = Dense(num_labels, activation='softmax')(y)\n","\n","# build the model by supplying inputs/outputs\n","model = Model(inputs=inputs, outputs=outputs)\n","\n","# classifier loss, Adam optimizer, classifier accuracy\n","model.compile(loss='categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","# train the model with input images and labels\n","model.fit(x_train,\n","          y_train,\n","          validation_data=(x_test, y_test),\n","          epochs=20,\n","          batch_size=batch_size)\n","\n","# model accuracy on test dataset\n","score = model.evaluate(x_test,\n","                       y_test,\n","                       batch_size=batch_size,\n","                       verbose=0)\n","print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"],"metadata":{"id":"EDhyrz0TRgY8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoFeatureExtractor, ResNetForImageClassification\n","import torch\n","from datasets import load_dataset\n","\n","dataset = load_dataset(\"huggingface/cats-image\")\n","image = dataset[\"test\"][\"image\"][0]\n","\n","feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-50\")\n","model = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n","\n","inputs = feature_extractor(image, return_tensors=\"pt\")\n","\n","with torch.no_grad():\n","    logits = model(**inputs).logits\n","\n","# model predicts one of the 1000 ImageNet classes\n","predicted_label = logits.argmax(-1).item()\n","print(model.config.id2label[predicted_label])"],"metadata":{"id":"DtzJgglF9yGD"},"execution_count":null,"outputs":[]}]}